{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906},{"sourceId":12488393,"sourceType":"datasetVersion","datasetId":7880616},{"sourceId":12496828,"sourceType":"datasetVersion","datasetId":7886686},{"sourceId":12499608,"sourceType":"datasetVersion","datasetId":7888742},{"sourceId":12503050,"sourceType":"datasetVersion","datasetId":7891018},{"sourceId":12577653,"sourceType":"datasetVersion","datasetId":7943464},{"sourceId":12579304,"sourceType":"datasetVersion","datasetId":7944422},{"sourceId":12581554,"sourceType":"datasetVersion","datasetId":7946065}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport glob\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom tifffile import imwrite\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\n\nscaler = MinMaxScaler()\n\nTRAIN_DATASET_PATH = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n\n# Prepare output folders\nos.makedirs(\"working/images\", exist_ok=True)\nos.makedirs(\"working/masks\", exist_ok=True)\n\n# List all modalities\nt1_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*t1.nii'))\nt2_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*t2.nii'))\nt1ce_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*t1ce.nii'))\nflair_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*flair.nii'))\nmask_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*seg.nii'))\n\n# Process first 60 patients\nfor img in range(60):\n    print(\"Now preparing image and masks number:\", img)\n\n    temp_image_t1 = nib.load(t1_list[img]).get_fdata()\n    temp_image_t1 = scaler.fit_transform(temp_image_t1.reshape(-1, temp_image_t1.shape[-1])).reshape(temp_image_t1.shape)\n\n    temp_image_t2 = nib.load(t2_list[img]).get_fdata()\n    temp_image_t2 = scaler.fit_transform(temp_image_t2.reshape(-1, temp_image_t2.shape[-1])).reshape(temp_image_t2.shape)\n\n    temp_image_t1ce = nib.load(t1ce_list[img]).get_fdata()\n    temp_image_t1ce = scaler.fit_transform(temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])).reshape(temp_image_t1ce.shape)\n\n    temp_image_flair = nib.load(flair_list[img]).get_fdata()\n    temp_image_flair = scaler.fit_transform(temp_image_flair.reshape(-1, temp_image_flair.shape[-1])).reshape(temp_image_flair.shape)\n\n    temp_mask = nib.load(mask_list[img]).get_fdata()\n    temp_mask = temp_mask.astype(np.uint8)\n    temp_mask[temp_mask == 4] = 3  # Reassign label 4 to 3\n\n    # Combine all 4 modalities (t1, flair, t1ce, t2)\n    temp_combined_images = np.stack([temp_image_t1, temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3)\n\n    # Crop to 128x128x128 (from center region)\n    temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n    temp_mask = temp_mask[56:184, 56:184, 13:141]\n\n    val, counts = np.unique(temp_mask, return_counts=True)\n    if (1 - (counts[0] / counts.sum())) > 0.01:  # At least 1% non-background\n        print(\"Save Me\")\n        temp_mask = to_categorical(temp_mask, num_classes=4)\n        np.save('/kaggle/working/working/images/image_' + str(img) + '.npy', temp_combined_images)\n        np.save('/kaggle/working/working/masks/mask_' + str(img) + '.npy', temp_mask)\n    else:\n        print(\"I am useless\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zip both folders into one zip file\n!zip -r /kaggle/working/output_data.zip /kaggle/working/working/images /kaggle/working/working/masks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'output_data.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport glob\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom tifffile import imwrite\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\n\nscaler = MinMaxScaler()\n\nTRAIN_DATASET_PATH = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n\n# Prepare output folders\nos.makedirs(\"working/images\", exist_ok=True)\nos.makedirs(\"working/masks\", exist_ok=True)\n\n# List all modalities\nt1_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*t1.nii'))\nt2_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*t2.nii'))\nt1ce_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*t1ce.nii'))\nflair_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*flair.nii'))\nmask_list = sorted(glob.glob(TRAIN_DATASET_PATH + '*/*seg.nii'))\n\n# Process first 60 patients\nfor img in range(60,121):\n    print(\"Now preparing image and masks number:\", img)\n\n    temp_image_t1 = nib.load(t1_list[img]).get_fdata()\n    temp_image_t1 = scaler.fit_transform(temp_image_t1.reshape(-1, temp_image_t1.shape[-1])).reshape(temp_image_t1.shape)\n\n    temp_image_t2 = nib.load(t2_list[img]).get_fdata()\n    temp_image_t2 = scaler.fit_transform(temp_image_t2.reshape(-1, temp_image_t2.shape[-1])).reshape(temp_image_t2.shape)\n\n    temp_image_t1ce = nib.load(t1ce_list[img]).get_fdata()\n    temp_image_t1ce = scaler.fit_transform(temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])).reshape(temp_image_t1ce.shape)\n\n    temp_image_flair = nib.load(flair_list[img]).get_fdata()\n    temp_image_flair = scaler.fit_transform(temp_image_flair.reshape(-1, temp_image_flair.shape[-1])).reshape(temp_image_flair.shape)\n\n    temp_mask = nib.load(mask_list[img]).get_fdata()\n    temp_mask = temp_mask.astype(np.uint8)\n    temp_mask[temp_mask == 4] = 3  # Reassign label 4 to 3\n\n    # Combine all 4 modalities (t1, flair, t1ce, t2)\n    temp_combined_images = np.stack([temp_image_t1, temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3)\n\n    # Crop to 128x128x128 (from center region)\n    temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n    temp_mask = temp_mask[56:184, 56:184, 13:141]\n\n    val, counts = np.unique(temp_mask, return_counts=True)\n    if (1 - (counts[0] / counts.sum())) > 0.01:  # At least 1% non-background\n        print(\"Save Me\")\n        temp_mask = to_categorical(temp_mask, num_classes=4)\n        np.save('/kaggle/working/working/images/image_' + str(img) + '.npy', temp_combined_images)\n        np.save('/kaggle/working/working/masks/mask_' + str(img) + '.npy', temp_mask)\n    else:\n        print(\"I am useless\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zip both folders into one zip file\n!zip -r /kaggle/working/output_data.zip /kaggle/working/working/images /kaggle/working/working/masks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'output_data.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/*\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport glob\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\n\n# Initialize scaler\nscaler = MinMaxScaler()\n\n# Dataset path\nTRAIN_DATASET_PATH = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n\n# Output directories\nos.makedirs(\"/kaggle/working/working/images\", exist_ok=True)\nos.makedirs(\"/kaggle/working/working/masks\", exist_ok=True)\n\n# Load sorted file paths\nt1_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*t1.nii*')))\nt2_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*t2.nii*')))\nt1ce_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*t1ce.nii*')))\nflair_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*flair.nii*')))\nmask_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*seg.nii*')))\n\n# Safety check\nmin_len = min(len(t1_list), len(t2_list), len(t1ce_list), len(flair_list), len(mask_list))\nprint(f\"✅ Number of patients: {min_len}\")\n\n# Process a safe range\nfor img in range(121, min(231, min_len)):\n    try:\n        print(\"Now preparing image and mask number:\", img)\n\n        # Load modalities\n        temp_image_t1 = nib.load(t1_list[img]).get_fdata()\n        temp_image_t2 = nib.load(t2_list[img]).get_fdata()\n        temp_image_t1ce = nib.load(t1ce_list[img]).get_fdata()\n        temp_image_flair = nib.load(flair_list[img]).get_fdata()\n        temp_mask = nib.load(mask_list[img]).get_fdata().astype(np.uint8)\n\n        # Normalize each modality\n        def normalize(img):\n            return scaler.fit_transform(img.reshape(-1, 1)).reshape(img.shape)\n\n        temp_image_t1 = normalize(temp_image_t1)\n        temp_image_t2 = normalize(temp_image_t2)\n        temp_image_t1ce = normalize(temp_image_t1ce)\n        temp_image_flair = normalize(temp_image_flair)\n\n        # Convert label 4 → 3\n        temp_mask[temp_mask == 4] = 3\n\n        # Stack modalities into one volume (H, W, D, 4)\n        temp_combined_images = np.stack([temp_image_t1, temp_image_flair, temp_image_t1ce, temp_image_t2], axis=-1)\n\n        # Center crop to 128×128×128\n        temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n        temp_mask = temp_mask[56:184, 56:184, 13:141]\n\n        # Check for foreground (at least 1% non-background)\n        val, counts = np.unique(temp_mask, return_counts=True)\n        foreground_ratio = 1 - (counts[0] / counts.sum()) if 0 in val else 1.0\n\n        if foreground_ratio > 0.01:\n            print(\"✅ Saving...\")\n            temp_mask = to_categorical(temp_mask, num_classes=4)\n            np.save(f\"/kaggle/working/working/images/image_{img}.npy\", temp_combined_images)\n            np.save(f\"/kaggle/working/working/masks/mask_{img}.npy\", temp_mask)\n        else:\n            print(\"⚠️ Skipped due to low foreground content\")\n\n    except Exception as e:\n        print(f\"❌ Failed at index {img} with error: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zip both folders into one zip file\n!zip -r /kaggle/working/output_data.zip /kaggle/working/working/images /kaggle/working/working/masks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'output_data.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport glob\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\n\n# Initialize scaler\nscaler = MinMaxScaler()\n\n# Dataset path\nTRAIN_DATASET_PATH = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n\n# Output directories\nos.makedirs(\"/kaggle/working/working/images\", exist_ok=True)\nos.makedirs(\"/kaggle/working/working/masks\", exist_ok=True)\n\n# Load sorted file paths\nt1_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*t1.nii*')))\nt2_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*t2.nii*')))\nt1ce_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*t1ce.nii*')))\nflair_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*flair.nii*')))\nmask_list = sorted(glob.glob(os.path.join(TRAIN_DATASET_PATH, '*/*seg.nii*')))\n\n# Safety check\nmin_len = min(len(t1_list), len(t2_list), len(t1ce_list), len(flair_list), len(mask_list))\nprint(f\"✅ Number of patients: {min_len}\")\n\n# Process a safe range\nfor img in range(231, min(369, min_len)):\n    try:\n        print(\"Now preparing image and mask number:\", img)\n\n        # Load modalities\n        temp_image_t1 = nib.load(t1_list[img]).get_fdata()\n        temp_image_t2 = nib.load(t2_list[img]).get_fdata()\n        temp_image_t1ce = nib.load(t1ce_list[img]).get_fdata()\n        temp_image_flair = nib.load(flair_list[img]).get_fdata()\n        temp_mask = nib.load(mask_list[img]).get_fdata().astype(np.uint8)\n\n        # Normalize each modality\n        def normalize(img):\n            return scaler.fit_transform(img.reshape(-1, 1)).reshape(img.shape)\n\n        temp_image_t1 = normalize(temp_image_t1)\n        temp_image_t2 = normalize(temp_image_t2)\n        temp_image_t1ce = normalize(temp_image_t1ce)\n        temp_image_flair = normalize(temp_image_flair)\n\n        # Convert label 4 → 3\n        temp_mask[temp_mask == 4] = 3\n\n        # Stack modalities into one volume (H, W, D, 4)\n        temp_combined_images = np.stack([temp_image_t1, temp_image_flair, temp_image_t1ce, temp_image_t2], axis=-1)\n\n        # Center crop to 128×128×128\n        temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n        temp_mask = temp_mask[56:184, 56:184, 13:141]\n\n        # Check for foreground (at least 1% non-background)\n        val, counts = np.unique(temp_mask, return_counts=True)\n        foreground_ratio = 1 - (counts[0] / counts.sum()) if 0 in val else 1.0\n\n        if foreground_ratio > 0.01:\n            print(\"✅ Saving...\")\n            temp_mask = to_categorical(temp_mask, num_classes=4)\n            np.save(f\"/kaggle/working/working/images/image_{img}.npy\", temp_combined_images)\n            np.save(f\"/kaggle/working/working/masks/mask_{img}.npy\", temp_mask)\n        else:\n            print(\"⚠️ Skipped due to low foreground content\")\n\n    except Exception as e:\n        print(f\"❌ Failed at index {img} with error: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zip both folders into one zip file\n!zip -r /kaggle/working/output_data.zip /kaggle/working/working/images /kaggle/working/working/masks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'output_data.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport glob\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom tifffile import imwrite\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nibabel as nib\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nVALIDATION_DATASET_PATH = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n\ntest_image_flair = nib.load(VALIDATION_DATASET_PATH + '/BraTS20_Validation_028/BraTS20_Validation_028_flair.nii').get_fdata()\ntest_image_t1 = nib.load(VALIDATION_DATASET_PATH + '/BraTS20_Validation_028/BraTS20_Validation_028_t1.nii').get_fdata()\ntest_image_t1ce = nib.load(VALIDATION_DATASET_PATH + '/BraTS20_Validation_028/BraTS20_Validation_028_t1ce.nii').get_fdata()\ntest_image_t2 = nib.load(VALIDATION_DATASET_PATH + '/BraTS20_Validation_028/BraTS20_Validation_028_t2.nii').get_fdata()\n\n\n# ✅ Define the slice number to view\nn_slice = test_image_flair.shape[2] // 2  # Middle slice\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(231)\nplt.imshow(test_image_flair[:, :, n_slice], cmap='gray')\nplt.title('Image flair')\n\nplt.subplot(232)\nplt.imshow(test_image_t1[:, :, n_slice], cmap='gray')\nplt.title('Image t1')\n\nplt.subplot(233)\nplt.imshow(test_image_t1ce[:, :, n_slice], cmap='gray')\nplt.title('Image t1ce')\n\nplt.subplot(234)\nplt.imshow(test_image_t2[:, :, n_slice], cmap='gray')\nplt.title('Image t2')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##################################################\n#PART 2: Explore the process of combining images to channels and divide them to patches\n#Includes...\n#Combining all 4 images to 4 channels of a numpy array.\n#\n################################################\n#Flair, T1CE, annd T2 have the most information\n#Combine t1ce, t2, and flair into single multichannel image\n\ncombined_x = np.stack([test_image_flair, test_image_t1ce, test_image_t2], axis=3)\n\n#Crop to a size to be divisible by 64 so we can later extract 64x64x64 patches. \n#cropping x, y, and z\n#combined_x=combined_x[24:216, 24:216, 13:141]\n\ncombined_x=combined_x[56:184, 56:184, 13:141] #Crop to 128x128x128x4\n\n\nplt.subplot(221)\nplt.imshow(combined_x[:,:,n_slice, 0], cmap='gray')\nplt.title('Image flair')\nplt.subplot(222)\nplt.imshow(combined_x[:,:,n_slice, 1], cmap='gray')\nplt.title('Image t1ce')\nplt.subplot(223)\nplt.imshow(combined_x[:,:,n_slice, 2], cmap='gray')\nplt.title('Image t2')\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport nibabel as nib\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\n\nscaler = MinMaxScaler()\n\n# Define modality lists (sorted to ensure consistent order)\nt1_list = sorted(glob.glob('/kaggle/input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/*/*t1.nii'))\nt2_list = sorted(glob.glob('/kaggle/input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/*/*t2.nii'))\nt1ce_list = sorted(glob.glob('/kaggle/input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/*/*t1ce.nii'))\nflair_list = sorted(glob.glob('/kaggle/input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/*/*flair.nii'))\n\n# Ensure output directory exists\nos.makedirs('/kaggle/working/working/images', exist_ok=True)\n\nfor img in range(1, 60):  # Loop over volume index\n    print(\"Now preparing image number:\", img)\n\n    # Load and normalize each modality\n    temp_image_t1 = nib.load(t1_list[img]).get_fdata()\n    temp_image_t1 = scaler.fit_transform(temp_image_t1.reshape(-1, temp_image_t1.shape[-1])).reshape(temp_image_t1.shape)\n\n    temp_image_t1ce = nib.load(t1ce_list[img]).get_fdata()\n    temp_image_t1ce = scaler.fit_transform(temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])).reshape(temp_image_t1ce.shape)\n\n    temp_image_t2 = nib.load(t2_list[img]).get_fdata()\n    temp_image_t2 = scaler.fit_transform(temp_image_t2.reshape(-1, temp_image_t2.shape[-1])).reshape(temp_image_t2.shape)\n\n    temp_image_flair = nib.load(flair_list[img]).get_fdata()\n    temp_image_flair = scaler.fit_transform(temp_image_flair.reshape(-1, temp_image_flair.shape[-1])).reshape(temp_image_flair.shape)\n\n    # Stack all four modalities: [H, W, D, 4]\n    temp_combined_images = np.stack([temp_image_flair, temp_image_t1, temp_image_t1ce, temp_image_t2], axis=3)\n\n    # Crop to shape divisible by 64: [128, 128, 128, 4]\n    temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]  # Shape: (128, 128, 128, 4)\n\n    # Replace this condition with an actual check on label usefulness if needed\n    # Dummy condition to simulate label content (e.g., check non-zero segmentation labels)\n    useful = True  # <-- Replace with actual condition if label mask is available\n\n    if useful:\n        print(\"Save Me\")\n        np.save(f'/kaggle/working/working/images/image_{img}.npy', temp_combined_images)\n    else:\n        print(\"I am useless\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/output_data.zip /kaggle/working/working/images ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'output_data.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print list lengths for debugging\nprint(\"t1_list:\", len(t1_list))\nprint(\"t1ce_list:\", len(t1ce_list))\nprint(\"t2_list:\", len(t2_list))\nprint(\"flair_list:\", len(flair_list))\n\n# Use the smallest length to avoid index errors\nnum_files = min(len(t1_list), len(t1ce_list), len(t2_list), len(flair_list))\n\nfor img in range(num_files):\n    print(\"Now preparing image number:\", img)\n\n    temp_image_t1 = nib.load(t1_list[img]).get_fdata()\n    temp_image_t1 = scaler.fit_transform(temp_image_t1.reshape(-1, temp_image_t1.shape[-1])).reshape(temp_image_t1.shape)\n\n    temp_image_t1ce = nib.load(t1ce_list[img]).get_fdata()\n    temp_image_t1ce = scaler.fit_transform(temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])).reshape(temp_image_t1ce.shape)\n\n    temp_image_t2 = nib.load(t2_list[img]).get_fdata()\n    temp_image_t2 = scaler.fit_transform(temp_image_t2.reshape(-1, temp_image_t2.shape[-1])).reshape(temp_image_t2.shape)\n\n    temp_image_flair = nib.load(flair_list[img]).get_fdata()\n    temp_image_flair = scaler.fit_transform(temp_image_flair.reshape(-1, temp_image_flair.shape[-1])).reshape(temp_image_flair.shape)\n\n    temp_combined_images = np.stack([temp_image_flair, temp_image_t1, temp_image_t1ce, temp_image_t2], axis=3)\n\n    temp_combined_images = temp_combined_images[56:184, 56:184, 13:141]\n\n    print(\"Saving image:\", img)\n    np.save(f'/kaggle/working/working/images/image_{img}.npy', temp_combined_images)\n\nprint(\"✅ Done preprocessing all validation volumes.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/output_data.zip /kaggle/working/working/images ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'output_data.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\n\ndef normalize_name(filename):\n    \"\"\"\n    Normalize image and mask filenames so that 'img001.npy' and 'mask001.npy' become '001'\n    \"\"\"\n    base = os.path.splitext(filename)[0]\n    digits = re.findall(r'\\d+', base)\n    return digits[0] if digits else base  # fallback to full name if no digits\n\ndef check_images_and_masks_with_mapping(image_dir, mask_dir):\n    image_files = [f for f in os.listdir(image_dir) if f.endswith('.npy')]\n    mask_files  = [f for f in os.listdir(mask_dir) if f.endswith('.npy')]\n\n    image_keys = set(normalize_name(f) for f in image_files)\n    mask_keys  = set(normalize_name(f) for f in mask_files)\n\n    print(f\"🧾 Found {len(image_files)} image files\")\n    print(f\"🧾 Found {len(mask_files)} mask files\")\n\n    missing_masks  = image_keys - mask_keys\n    missing_images = mask_keys - image_keys\n\n    if not missing_masks and not missing_images:\n        print(\"✅ All image and mask pairs match (based on numeric ID)!\")\n    else:\n        if missing_masks:\n            print(f\"❌ Missing masks for {len(missing_masks)} image(s):\")\n            for key in sorted(missing_masks):\n                print(f\"  - img{key}.npy\")\n\n        if missing_images:\n            print(f\"❌ Missing images for {len(missing_images)} mask(s):\")\n            for key in sorted(missing_images):\n                print(f\"  - mask{key}.npy\")\n\n# Update these paths\nimage_dir = '/kaggle/input/training/training/images'\nmask_dir  = '/kaggle/input/training/training/masks'\n\ncheck_images_and_masks_with_mapping(image_dir, mask_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\n\ndef normalize_name(filename):\n    \"\"\"\n    Normalize image and mask filenames so that 'img001.npy' and 'mask001.npy' become '001'\n    \"\"\"\n    base = os.path.splitext(filename)[0]\n    digits = re.findall(r'\\d+', base)\n    return digits[0] if digits else base  # fallback to full name if no digits\n\ndef check_images_and_masks_with_mapping(image_dir, mask_dir):\n    image_files = [f for f in os.listdir(image_dir) if f.endswith('.npy')]\n    mask_files  = [f for f in os.listdir(mask_dir) if f.endswith('.npy')]\n\n    image_keys = set(normalize_name(f) for f in image_files)\n    mask_keys  = set(normalize_name(f) for f in mask_files)\n\n    print(f\"🧾 Found {len(image_files)} image files\")\n    print(f\"🧾 Found {len(mask_files)} mask files\")\n\n    missing_masks  = image_keys - mask_keys\n    missing_images = mask_keys - image_keys\n\n    if not missing_masks and not missing_images:\n        print(\"✅ All image and mask pairs match (based on numeric ID)!\")\n    else:\n        if missing_masks:\n            print(f\"❌ Missing masks for {len(missing_masks)} image(s):\")\n            for key in sorted(missing_masks):\n                print(f\"  - img{key}.npy\")\n\n        if missing_images:\n            print(f\"❌ Missing images for {len(missing_images)} mask(s):\")\n            for key in sorted(missing_images):\n                print(f\"  - mask{key}.npy\")\n\n# Update these paths\nimage_dir = '/kaggle/input/testing/testing/images'\nmask_dir  = '/kaggle/input/testing/testing/masks'\n\ncheck_images_and_masks_with_mapping(image_dir, mask_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.models import Model, load_model\nfrom keras.layers import (Input, Conv3D, MaxPooling3D, concatenate,\n                          UpSampling3D, Dropout, LeakyReLU, GlobalAveragePooling3D,\n                          Reshape, Dense, Multiply)\nfrom keras.layers import LayerNormalization as InstanceNormalization\nfrom keras import regularizers\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\n# ============================================\n# Squeeze-and-Excitation Block\n# ============================================\n=\n# ============================================\n# Load Pretrained or Initialize New Model\n# ============================================\ninput_model_path = \"/kaggle/input/train-1-2/latup_attention_model_1_2.h5\"\noutput_model_path = \"/kaggle/working/latup_attention_model_1_3.h5\"\n\nif os.path.exists(input_model_path):\n    print(\"✅ Loading pre-trained model from Kaggle input...\")\n    model = load_model(input_model_path, custom_objects={\n        \"loss\": dice_ce_loss()\n    })\nelse:\n    print(\"🚀 Creating new LATUP + Attention + SE model...\")\n    model = latup_attention_unet(128, 128, 128, 4, 4)\n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=dice_ce_loss(), metrics=['accuracy'])\n\n# ============================================\n# Training Setup\n# ============================================\ntrain_img_dir = \"/kaggle/input/training-1111/training/images\"\ntrain_mask_dir = \"/kaggle/input/training-1111/training/masks\"\nbatch_size = 2\ntrain_generator = imageLoader(train_img_dir, train_mask_dir, batch_size)\nsteps_per_epoch = len(os.listdir(train_img_dir)) // batch_size\n\ncheckpoint = ModelCheckpoint(output_model_path, monitor='loss', save_best_only=True, verbose=1)\nlr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1)\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=50,\n    callbacks=[checkpoint, lr_scheduler],\n    verbose=1\n)\n\n# ============================================\n# Plotting\n# ============================================\nplt.figure(figsize=(10, 4))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 4))\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.title(\"Accuracy Curve\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# LATUP‑Net – Evaluation Script\n# ===============================\nimport os, glob, numpy as np, tensorflow as tf\nfrom tensorflow.keras.models import load_model\nimport pandas as pd\n\n# --------------------------------------------------\n# 1.  CUSTOM LOSS (needed only for model loading)\n# --------------------------------------------------\ndef dice_ce_loss(smooth=1e-6):\n    def loss(y_true, y_pred):\n        weights = tf.constant([0, 0.4, 0.25, 0.45], dtype=tf.float32)\n        y_true  = tf.cast(y_true, tf.float32)\n        y_pred  = tf.cast(y_pred, tf.float32)\n        y_pred  = tf.clip_by_value(\n            y_pred, tf.keras.backend.epsilon(), 1. - tf.keras.backend.epsilon()\n        )\n        ce = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true, y_pred))\n        dice_terms = []\n        for c in range(y_true.shape[-1]):\n            inter = tf.reduce_sum(y_true[..., c] * y_pred[..., c])\n            denom = tf.reduce_sum(y_true[..., c] + y_pred[..., c])\n            dice  = (2. * inter + smooth) / (denom + smooth)a\n            dice_terms.append((1 - dice) * weights[c])\n        dice_loss = tf.reduce_sum(dice_terms)\n        return dice_loss + ce\n    return loss\n\n# --------------------------------------------------\n# 2.  LOAD TRAINED MODEL\n# --------------------------------------------------\n\nmodel = load_model(\n    \"/kaggle/input/train-1-30/latup_attention_model_1_3 (2).h5\",\n    custom_objects={\"dice_ce_loss\": dice_ce_loss},\n    compile=False\n)\n\nmodel.trainable = False  # keep everything in inference mode\n\n# --------------------------------------------------\n# 3.  PRE‑ / POST‑PROCESS HELPERS\n# --------------------------------------------------\ndef zscore(volume):\n    \"\"\"Per‑modality z‑normalisation for a 4‑D MRI block (x,y,z,channels).\"\"\"\n    vol = volume.astype(np.float32)\n    for c in range(vol.shape[-1]):\n        mu  = vol[..., c].mean()\n        sig = vol[..., c].std()\n        vol[..., c] = (vol[..., c] - mu) / (sig + 1e-8)\n    return vol\n\ndef dice_coef(mask_gt, mask_pred, label, smooth=1e-6):\n    \"\"\"Plain single‑label Dice (inputs are binary: 0/1).\"\"\"\n    gt   = (mask_gt  == label).astype(np.float32)\n    pred = (mask_pred == label).astype(np.float32)\n    inter = np.sum(gt * pred)\n    denom = np.sum(gt) + np.sum(pred)\n    return (2 * inter + smooth) / (denom + smooth)\n\ndef composite_labels(mask):\n    \"\"\"Return 3 composite binary masks for WT, TC, ET.\"\"\"\n    # BraTS labels: 0 = bg, 1 = edema, 2 = non‑enh core, 3 = enh core\n    wt = np.isin(mask, [1,2,3])\n    tc = np.isin(mask, [2,3])\n    et = (mask == 3)\n    return wt, tc, et\n\n# --------------------------------------------------\n# 4.  DATA LOADING (same folder layout as training)\n# --------------------------------------------------\ntest_img_dir  = \"/kaggle/input/testing/testing/images\"\ntest_mask_dir = \"/kaggle/input/testing/testing/masks\"\n\nimage_paths = sorted(glob.glob(os.path.join(test_img_dir,  \"*.npy\")))\nmask_paths  = sorted(glob.glob(os.path.join(test_mask_dir, \"*.npy\")))\nassert len(image_paths) == len(mask_paths), \"Image/mask count mismatch\"\n\nresults = []   # will become a dataframe later\n\n   # --------------------------------------------------\n# 5. INFERENCE LOOP\n# --------------------------------------------------\nfor img_p, msk_p in zip(image_paths, mask_paths):\n    case_id = os.path.basename(img_p).replace(\".npy\", \"\")\n    img  = np.load(img_p)   # (X,Y,Z,4)\n    mask = np.load(msk_p)   # (X,Y,Z) or one-hot\n\n    if mask.ndim == 4:  # one-hot to label\n        mask = np.argmax(mask, axis=-1)\n\n    img_norm = zscore(img)[None, ...]\n    pred_prob = model.predict(img_norm, verbose=0)[0]\n    pred_lbl  = np.argmax(pred_prob, axis=-1)\n\n    # Label‑wise Dice scores\n    d_bg  = dice_coef(mask, pred_lbl, 0)\n    d_ed  = dice_coef(mask, pred_lbl, 1)  # Edema\n    d_nec = dice_coef(mask, pred_lbl, 2)  # Necrosis (non-enh core)\n    d_et  = dice_coef(mask, pred_lbl, 3)  # Enhancing tumor\n\n    # Composite Dice scores\n    wt_gt, tc_gt, et_gt = composite_labels(mask)\n    wt_pr, tc_pr, et_pr = composite_labels(pred_lbl)\n    d_wt = dice_coef(wt_gt, wt_pr, True)\n    d_tc = dice_coef(tc_gt, tc_pr, True)\n    d_enh = dice_coef(et_gt, et_pr, True)\n\n    results.append(dict(\n        case=case_id,\n        Dice_BG=d_bg,\n        Dice_Edema=d_ed,\n        Dice_Necrosis=d_nec,\n        Dice_EnhancingTumor=d_et,\n        Dice_WT=d_wt,\n        Dice_TC=d_tc,\n        Dice_ET=d_enh\n    ))\n\n    print(f\"{case_id}: WT {d_wt:.4f} | TC {d_tc:.4f} | ET {d_enh:.4f} | BG {d_bg:.4f} | ED {d_ed:.4f} | NEC {d_nec:.4f} | ETum {d_et:.4f}\")\n\n# --------------------------------------------------\n# 6. SUMMARY REPORTING\n# --------------------------------------------------\ndf = pd.DataFrame(results)\n\nif df.empty:\n    print(\"\\n❌ No cases processed. Check file paths/extensions.\")\nelse:\n    composite_metrics = [\"Dice_WT\", \"Dice_TC\", \"Dice_ET\"]\n    label_metrics     = [\"Dice_BG\", \"Dice_Edema\", \"Dice_Necrosis\", \"Dice_EnhancingTumor\"]\n\n    print(\"\\n=== Mean Dice Scores: Composite Regions ===\")\n    print(df[composite_metrics].mean())\n\n    print(\"\\n=== Mean Dice Scores: Per-Class ===\")\n    print(df[label_metrics].mean())\n\n    print(\"\\nDetailed per-case Dice table:\")\n    print(df)\n\n    # Optional CSV export\n    # df.to_csv(\"latupnet_dice_scores.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\ndef squeeze_excite_block(input_tensor, ratio=8):\n    filters = input_tensor.shape[-1]\n    se = layers.GlobalAveragePooling3D()(input_tensor)\n    se = layers.Dense(filters // ratio, activation='relu')(se)\n    se = layers.Dense(filters, activation='sigmoid')(se)\n    se = layers.Reshape((1, 1, 1, filters))(se)\n    return layers.multiply([input_tensor, se])\n\ndef conv_block(x, filters, kernel_size=3, dropout=0.2):\n    x = layers.Conv3D(filters, kernel_size, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    x = squeeze_excite_block(x)\n    x = layers.Dropout(dropout)(x)\n    return x\n\ndef attention_gate(x, g, inter_channels):\n    theta_x = layers.Conv3D(inter_channels, 1)(x)\n    phi_g = layers.Conv3D(inter_channels, 1)(g)\n    add = layers.Add()([theta_x, phi_g])\n    relu = layers.Activation('relu')(add)\n    psi = layers.Conv3D(1, 1, activation='sigmoid')(relu)\n    return layers.multiply([x, psi])\n\ndef encoder_block(x, filters):\n    c = conv_block(x, filters)\n    p = layers.MaxPooling3D((2, 2, 2))(c)\n    return c, p\n\ndef decoder_block(x, skip, filters):\n    us = layers.UpSampling3D((2, 2, 2))(x)\n    att = attention_gate(skip, us, filters // 2)\n    concat = layers.Concatenate()([us, att])\n    c = conv_block(concat, filters)\n    return c\n\ndef get_latup_attention_model(input_shape=(64, 64, 64, 4), num_classes=4):\n    inputs = layers.Input(input_shape)\n\n    # Encoder\n    c1, p1 = encoder_block(inputs, 32)\n    c2, p2 = encoder_block(p1, 64)\n    c3, p3 = encoder_block(p2, 128)\n    c4, p4 = encoder_block(p3, 256)\n\n    # Bottleneck\n    bn = conv_block(p4, 512)\n\n    # Decoder\n    d4 = decoder_block(bn, c4, 256)\n    d3 = decoder_block(d4, c3, 128)\n    d2 = decoder_block(d3, c2, 64)\n    d1 = decoder_block(d2, c1, 32)\n\n    outputs = layers.Conv3D(num_classes, 1, activation='softmax')(d1)\n    return Model(inputs, outputs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\ndef squeeze_excite_block(input_tensor, ratio=8):\n    filters = input_tensor.shape[-1]\n    se = layers.GlobalAveragePooling3D()(input_tensor)\n    se = layers.Dense(filters // ratio, activation='relu')(se)\n    se = layers.Dense(filters, activation='sigmoid')(se)\n    se = layers.Reshape((1, 1, 1, filters))(se)\n    return layers.multiply([input_tensor, se])\n\ndef conv_block(x, filters, kernel_size=3, dropout=0.2):\n    x = layers.Conv3D(filters, kernel_size, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    x = squeeze_excite_block(x)\n    x = layers.Dropout(dropout)(x)\n    return x\n\ndef attention_gate(x, g, inter_channels):\n    theta_x = layers.Conv3D(inter_channels, 1)(x)\n    phi_g = layers.Conv3D(inter_channels, 1)(g)\n    add = layers.Add()([theta_x, phi_g])\n    relu = layers.Activation('relu')(add)\n    psi = layers.Conv3D(1, 1, activation='sigmoid')(relu)\n    return layers.multiply([x, psi])\n\ndef encoder_block(x, filters):\n    c = conv_block(x, filters)\n    p = layers.MaxPooling3D((2, 2, 2))(c)\n    return c, p\n\ndef decoder_block(x, skip, filters):\n    us = layers.UpSampling3D((2, 2, 2))(x)\n    att = attention_gate(skip, us, filters // 2)\n    concat = layers.Concatenate()([us, att])\n    c = conv_block(concat, filters)\n    return c\n\ndef get_latup_attention_model(input_shape=(64, 64, 64, 4), num_classes=4):\n    inputs = layers.Input(input_shape)\n\n    # Encoder\n    c1, p1 = encoder_block(inputs, 32)\n    c2, p2 = encoder_block(p1, 64)\n    c3, p3 = encoder_block(p2, 128)\n    c4, p4 = encoder_block(p3, 256)\n\n    # Bottleneck\n    bn = conv_block(p4, 512)\n\n    # Decoder\n    d4 = decoder_block(bn, c4, 256)\n    d3 = decoder_block(d4, c3, 128)\n    d2 = decoder_block(d3, c2, 64)\n    d1 = decoder_block(d2, c1, 32)\n\n    outputs = layers.Conv3D(num_classes, 1, activation='softmax')(d1)\n    return Model(inputs, outputs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef dice_loss(y_true, y_pred, smooth=1e-5):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return 1 - (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_ce_loss(y_true, y_pred):\n    ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n    d = dice_loss(y_true, y_pred)\n    return ce + d\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q tensorflow-addons\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import Conv3D, MaxPooling3D, Conv3DTranspose, Concatenate, Activation, BatchNormalization, multiply, add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\nfrom tensorflow.keras import mixed_precision\nfrom sklearn.utils import shuffle\n\n# Enable mixed precision\nmixed_precision.set_global_policy('mixed_float16')\n\ndef attention_gate(x, g, inter_shape):\n    theta_x = Conv3D(inter_shape, 1, strides=1, padding='same')(x)\n    phi_g = Conv3D(inter_shape, 1, strides=1, padding='same')(g)\n    add_xg = add([theta_x, phi_g])\n    act_xg = Activation('relu')(add_xg)\n    psi = Conv3D(1, 1, strides=1, padding='same')(act_xg)\n    sigmoid_xg = Activation('sigmoid')(psi)\n    return multiply([x, sigmoid_xg])\n\ndef se_block(input_tensor, compress_ratio=16):\n    channels = input_tensor.shape[-1]\n    se_shape = (1, 1, 1, channels)\n    se = tf.keras.layers.GlobalAveragePooling3D()(input_tensor)\n    se = tf.keras.layers.Reshape(se_shape)(se)\n    se = tf.keras.layers.Dense(channels // compress_ratio, activation='relu')(se)\n    se = tf.keras.layers.Dense(channels, activation='sigmoid')(se)\n    return multiply([input_tensor, se])\n\ndef conv_block(x, filters):\n    x = Conv3D(filters, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv3D(filters, 3, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\ndef encoder_block(x, filters):\n    c = conv_block(x, filters)\n    p = MaxPooling3D((2, 2, 2))(c)\n    return c, p\n\ndef decoder_block(x, skip, filters):\n    g = Conv3DTranspose(filters, (2, 2, 2), strides=(2, 2, 2), padding='same')(x)\n    att = attention_gate(skip, g, filters)\n    concat = Concatenate()([g, att])\n    c = conv_block(concat, filters)\n    return c\n\ndef latup_attention_unet(img_depth, img_height, img_width, img_channels, num_classes):\n    inputs = Input((img_depth, img_height, img_width, img_channels))\n\n    enc1, pool1 = encoder_block(inputs, 32)\n    enc2, pool2 = encoder_block(pool1, 64)\n    enc3, pool3 = encoder_block(pool2, 128)\n    enc4, pool4 = encoder_block(pool3, 256)\n\n    center = conv_block(pool4, 512)\n\n    dec4 = decoder_block(center, enc4, 256)\n    dec3 = decoder_block(dec4, enc3, 128)\n    dec2 = decoder_block(dec3, enc2, 64)\n    dec1 = decoder_block(dec2, enc1, 32)\n\n    outputs = Conv3D(num_classes, 1, activation='softmax', dtype='float32')(dec1)\n\n    model = Model(inputs, outputs)\n    return model\n\n# Custom Dice + CE Loss\nimport tensorflow.keras.backend as K\n\ndef dice_loss(y_true, y_pred, smooth=1e-6):\n    y_true_f = K.flatten(tf.one_hot(tf.cast(y_true, tf.int32), tf.shape(y_pred)[-1]))\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return 1 - (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_ce_loss(y_true, y_pred):\n    ce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n    d_loss = dice_loss(y_true, y_pred)\n    return ce + d_loss\n\n# Load .npy images and masks from directories\ndef load_data(image_dir, mask_dir):\n    images, masks = [], []\n    image_filenames = sorted(os.listdir(image_dir))\n    mask_filenames = sorted(os.listdir(mask_dir))\n\n    for img_name, mask_name in zip(image_filenames, mask_filenames):\n        img_path = os.path.join(image_dir, img_name)\n        mask_path = os.path.join(mask_dir, mask_name)\n\n        img = np.load(img_path).astype(np.float32)\n        mask = np.load(mask_path).astype(np.uint8)\n\n        images.append(img)\n        masks.append(mask)\n\n    return np.array(images), np.array(masks)\n\n# Generator function\ndef data_generator(images, masks, batch_size):\n    while True:\n        images, masks = shuffle(images, masks)\n        for i in range(0, len(images), batch_size):\n            yield images[i:i+batch_size], masks[i:i+batch_size]\n\n# Paths and settings\nimage_dir = \"/kaggle/input/training-1111/training/images\"\nmask_dir = \"/kaggle/input/training-1111/training/masks\"\n\nimages, masks = load_data(image_dir, mask_dir)\n\nbatch_size = 1\ntrain_generator = data_generator(images, masks, batch_size=batch_size)\n\nsteps_per_epoch = len(images) // batch_size\ninput_shape = images[0].shape\nnum_classes = masks.max() + 1\n\nmodel_save_path = \"best_model_1.h5\"\nmodel = latup_attention_unet(*input_shape, num_classes)\n\n# Load pre-trained weights if available\npretrained_path = \"/kaggle/input/train-1-30/latup_attention_model_1_3 (2).h5\"\nif os.path.exists(pretrained_path):\n    model.load_weights(pretrained_path)\n    print(\"Loaded pre-trained model weights.\")\n\noptimizer = Adam(learning_rate=1e-4)\nmodel.compile(optimizer=optimizer, loss=dice_ce_loss, metrics=['accuracy'])\n\ncheckpoint = ModelCheckpoint(model_save_path, monitor='loss', save_best_only=True, verbose=1)\nearly_stop = EarlyStopping(monitor='loss', patience=10, verbose=1)\ncsv_logger = CSVLogger('training_log.csv')\nlr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1)\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=50,\n    callbacks=[checkpoint, early_stop, csv_logger, lr_scheduler],\n    verbose=1\n)\n\n# Save final model\nmodel.save(\"final_model.h5\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# Visualization of Predictions vs Ground Truth\n# ============================================\n\nimport os\nimport numpy as np\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import Conv3D, MaxPooling3D, Conv3DTranspose, Concatenate, Activation, BatchNormalization, multiply, add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\nfrom tensorflow.keras import mixed_precision\nfrom sklearn.utils import shuffle\n\n\n\n\ndef squeeze_excitation_block(input_tensor, ratio=16):\n    channel_axis = -1\n    filters = input_tensor.shape[channel_axis]\n    se = GlobalAveragePooling3D()(input_tensor)\n    se = Reshape((1, 1, 1, filters))(se)\n    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal')(se)\n    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal')(se)\n    return Multiply()([input_tensor, se])\n\n# ============================================\n# Parallel Convolution Block (First Encoder Block)\n# ============================================\ndef parallel_conv_block(input_tensor, filters):\n    shared = Conv3D(filters, kernel_size=3, padding='same', activation='relu')(input_tensor)\n    path1 = Conv3D(filters, kernel_size=1, padding='same', activation='relu')(shared)\n    path1 = MaxPooling3D(pool_size=(2, 2, 2))(path1)\n    path2 = Conv3D(filters, kernel_size=3, padding='same', activation='relu')(shared)\n    path2 = MaxPooling3D(pool_size=(2, 2, 2))(path2)\n    path3 = Conv3D(filters, kernel_size=5, padding='same', activation='relu')(shared)\n    path3 = MaxPooling3D(pool_size=(2, 2, 2))(path3)\n    return concatenate([path1, path2, path3], axis=-1)\n\n# ============================================\n# LATUP-Net Architecture\n# ============================================\ndef latup_attention_unet(x, y, z, channels, num_classes):\n    inputs = Input((x, y, z, channels))\n    enc1 = parallel_conv_block(inputs, 32)\n    enc2 = squeeze_excitation_block(enc1)\n    enc2 = Conv3D(64, 3, padding='same', kernel_regularizer=regularizers.l2(0.02))(enc2)\n    enc2 = InstanceNormalization()(enc2)\n    enc2 = LeakyReLU(alpha=0.1)(enc2)\n    enc2 = Conv3D(64, 3, padding='same', kernel_regularizer=regularizers.l2(0.02))(enc2)\n    enc2 = InstanceNormalization()(enc2)\n    enc2 = LeakyReLU(alpha=0.1)(enc2)\n    enc2 = Dropout(0.2)(enc2)\n    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(enc2)\n\n    enc3 = squeeze_excitation_block(pool2)\n    enc3 = Conv3D(128, 3, padding='same', kernel_regularizer=regularizers.l2(0.02))(enc3)\n    enc3 = InstanceNormalization()(enc3)\n    enc3 = LeakyReLU(alpha=0.1)(enc3)\n    enc3 = Conv3D(128, 3, padding='same', kernel_regularizer=regularizers.l2(0.02))(enc3)\n    enc3 = InstanceNormalization()(enc3)\n    enc3 = LeakyReLU(alpha=0.1)(enc3)\n    enc3 = Dropout(0.2)(enc3)\n    pool3 = MaxPooling3D(pool_size=(2, 2, 2))(enc3)\n\n    bn = squeeze_excitation_block(pool3)\n\n    up3 = UpSampling3D(size=(2, 2, 2))(bn)\n    dec3 = concatenate([up3, enc3])\n    dec3 = Conv3D(128, 3, padding='same')(dec3)\n    dec3 = InstanceNormalization()(dec3)\n    dec3 = LeakyReLU(alpha=0.1)(dec3)\n    dec3 = squeeze_excitation_block(dec3)\n    dec3 = Conv3D(128, 3, padding='same')(dec3)\n\n    up2 = UpSampling3D(size=(2, 2, 2))(dec3)\n    dec2 = concatenate([up2, enc2])\n    dec2 = Conv3D(64, 3, padding='same')(dec2)\n    dec2 = InstanceNormalization()(dec2)\n    dec2 = LeakyReLU(alpha=0.1)(dec2)\n    dec2 = squeeze_excitation_block(dec2)\n    dec2 = Conv3D(64, 3, padding='same')(dec2)\n\n    up1 = UpSampling3D(size=(2, 2, 2))(dec2)\n    dec1 = Conv3D(32, 3, padding='same')(up1)\n    dec1 = concatenate([dec1, inputs])\n    dec1 = Conv3D(32, 3, padding='same')(dec1)\n\n    outputs = Conv3D(num_classes, 1, activation='softmax')(dec1)\n    return omdel(inputs=[inputs], outputs=[outputs])\n\ntrain_img_dir = \"/kaggle/input/training-1111/training/images\"\ntrain_mask_dir = \"/kaggle/input/training-1111/training/masks\"\n\ndef visualize_prediction(model, image_path, mask_path, slice_idx=64):\n    # Load image and mask\n    image = np.load(image_path).astype(np.float32)\n    mask = np.load(mask_path).astype(np.uint8)\n\n    # Normalize image\n    for c in range(image.shape[-1]):\n        mean = np.mean(image[..., c])\n        std = np.std(image[..., c])\n        image[..., c] = (image[..., c] - mean) / (std + 1e-8)\n\n    # Add batch dimension\n    image_batch = np.expand_dims(image, axis=0)\n    \n    # Predict\n    prediction = model.predict(image_batch)\n    prediction = np.argmax(prediction[0], axis=-1)\n\n    # Process ground truth\n    if mask.ndim == 4:\n        mask = np.argmax(mask, axis=-1)\n\n    # Plot\n    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n    \n    axs[0].imshow(image[..., 0][..., slice_idx], cmap='gray')\n    axs[0].set_title(\"Input MRI (Modality 0)\")\n\n    axs[1].imshow(mask[..., slice_idx], cmap='tab10', vmin=0, vmax=3)\n    axs[1].set_title(\"Ground Truth Mask\")\n\n    axs[2].imshow(prediction[..., slice_idx], cmap='tab10', vmin=0, vmax=3)\n    axs[2].set_title(\"Predicted Mask\")\n\n    for ax in axs:\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n# ============================================\n# Example Visualization\n# ============================================\nsample_img_path = os.path.join(train_img_dir, sorted(os.listdir(train_img_dir))[0])\nsample_mask_path = os.path.join(train_mask_dir, sorted(os.listdir(train_mask_dir))[0])\n\nvisualize_prediction(model, sample_img_path, sample_mask_path, slice_idx=64)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import load_model\n\n# ============================================\n# Directories for training images and masks\n# ============================================\ntrain_img_dir = \"/kaggle/input/training-1111/training/images\"\ntrain_mask_dir = \"/kaggle/input/training-1111/training/masks\"\n\n# ============================================\n# Load a single sample from training data\n# ============================================\nsample_img_filename = os.listdir(train_img_dir)[0]\nsample_mask_filename = os.listdir(train_mask_dir)[0]\n\nsample_img_path = os.path.join(train_img_dir, sample_img_filename)\nsample_mask_path = os.path.join(train_mask_dir, sample_mask_filename)\n\n# Load .npy files (assuming shape: (128, 128, 128, 4) for image, and (128, 128, 128, num_classes) or (128, 128, 128) for mask)\nsample_img = np.load(sample_img_path)      # shape: (128, 128, 128, 4)\nsample_mask = np.load(sample_mask_path)    # shape: (128, 128, 128, num_classes) or (128, 128, 128)\n\n# Normalize image\nsample_img = sample_img.astype('float32') / np.max(sample_img)\n\n# Add batch dimension for model input\nsample_input = np.expand_dims(sample_img, axis=0)  # shape: (1, 128, 128, 128, 4)\n\n# ============================================\n# Load the trained model\n# ============================================\ninput_model_path = \"/kaggle/input/train-1-30/latup_attention_model_1_3 (2).h5\"\nmodel = load_model(input_model_path, compile=False)  # Assuming loss is custom, skip compile\n\n# ============================================\n# Predict the mask\n# ============================================\npred_mask = model.predict(sample_input)              # shape: (1, 128, 128, 128, num_classes)\npred_mask = np.argmax(pred_mask, axis=-1)[0]         # shape: (128, 128, 128)\n\n# If ground truth mask is one-hot encoded\nif sample_mask.ndim == 4:\n    sample_mask = np.argmax(sample_mask, axis=-1)    # shape: (128, 128, 128)\n\n# ============================================\n# Choose a slice for visualization\n# ============================================\nmid_slice = sample_img.shape[2] // 2  # Slice 64\n\n# ============================================\n# Plot Input, Ground Truth, and Prediction\n# ============================================\nplt.figure(figsize=(18, 5))\n\n# Show T1 slice (assumed channel 0)\nplt.subplot(1, 3, 1)\nplt.imshow(sample_img[:, :, mid_slice, 0], cmap='gray')\nplt.title(\"Input Image (T1)\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(sample_mask[:, :, mid_slice], cmap='jet', vmin=0, vmax=3)\nplt.title(\"Ground Truth Mask\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(pred_mask[:, :, mid_slice], cmap='jet', vmin=0, vmax=3)\nplt.title(\"Predicted Mask\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import load_model\n\n# ============================================\n# Paths to Data\n# ============================================\ntrain_img_dir = \"/kaggle/input/training-1111/training/images\"\ntrain_mask_dir = \"/kaggle/input/training-1111/training/masks\"\ninput_model_path = \"/kaggle/input/train-1-30/latup_attention_model_1_3 (2).h5\"\n\n# ============================================\n# Load Model (Skip compile if using custom loss)\n# ============================================\nmodel = load_model(input_model_path, compile=False)\n\n# ============================================\n# Loop Over All Images and Masks\n# ============================================\nimage_files = sorted(os.listdir(train_img_dir))\nmask_files = sorted(os.listdir(train_mask_dir))\n\nfor img_file, mask_file in zip(image_files, mask_files):\n    # Load image and mask\n    img_path = os.path.join(train_img_dir, img_file)\n    mask_path = os.path.join(train_mask_dir, mask_file)\n\n    img = np.load(img_path).astype('float32')       # shape: (128, 128, 128, 4)\n    mask = np.load(mask_path)                       # shape: (128, 128, 128, num_classes) or (128, 128, 128)\n\n    # Normalize image\n    img /= np.max(img)\n\n    # Add batch dimension\n    input_tensor = np.expand_dims(img, axis=0)      # shape: (1, 128, 128, 128, 4)\n\n    # Predict\n    pred = model.predict(input_tensor)\n    pred = np.argmax(pred, axis=-1)[0]              # shape: (128, 128, 128)\n\n    # Convert one-hot mask to class labels if needed\n    if mask.ndim == 4:\n        mask = np.argmax(mask, axis=-1)             # shape: (128, 128, 128)\n\n    # Choose middle slice (axial)\n    slice_idx = img.shape[2] // 2\n\n    # ============================================\n    # Plot\n    # ============================================\n    plt.figure(figsize=(18, 5))\n    plt.suptitle(f\"Filename: {img_file}\", fontsize=14)\n\n    # T1 modality\n    plt.subplot(1, 3, 1)\n    plt.imshow(img[:, :, slice_idx, 0], cmap='gray')\n    plt.title(\"Input Image (T1)\")\n    plt.axis('off')\n\n    # Ground truth\n    plt.subplot(1, 3, 2)\n    plt.imshow(mask[:, :, slice_idx], cmap='jet', vmin=0, vmax=3)\n    plt.title(\"Ground Truth Mask\")\n    plt.axis('off')\n\n    # Predicted\n    plt.subplot(1, 3, 3)\n    plt.imshow(pred[:, :, slice_idx], cmap='jet', vmin=0, vmax=3)\n    plt.title(\"Predicted Mask\")\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, Conv3DTranspose, Concatenate, BatchNormalization, Activation, Dropout, Add, Multiply\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import Sequence\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_img_dir = \"/kaggle/input/training-1111/training/images\"\ntrain_mask_dir = \"/kaggle/input/training-1111/training/masks\"\ninput_model_path = \"/kaggle/input/train-1-30/latup_attention_model_1_3 (2).h5\"\noutput_model_path = \"/kaggle/working/latup_attention_model_trained_2.h5\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_loss(y_true, y_pred, smooth=1e-5):\n    y_true_f = tf.keras.backend.flatten(y_true)\n    y_pred_f = tf.keras.backend.flatten(y_pred)\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\ndef dice_ce_loss():\n    def loss(y_true, y_pred):\n        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n        d = dice_loss(y_true, y_pred)\n        return ce + d\n    return loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def se_block(input_tensor, reduction=16):\n    channels = input_tensor.shape[-1]\n    se = GlobalAveragePooling3D()(input_tensor)\n    se = Dense(channels // reduction, activation='relu')(se)\n    se = Dense(channels, activation='sigmoid')(se)\n    se = Reshape((1, 1, 1, channels))(se)\n    return Multiply()([input_tensor, se])\n\ndef latup_attention_unet(x, y, z, in_channels, out_classes):\n    inputs = Input((x, y, z, in_channels))\n\n    def conv_block(x, filters):\n        x = Conv3D(filters, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Conv3D(filters, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        return x\n\n    def encoder_block(x, filters):\n        c = conv_block(x, filters)\n        p = MaxPooling3D(pool_size=(2, 2, 2))(c)\n        return c, p\n\n    def decoder_block(x, skip, filters):\n        up = Conv3DTranspose(filters, 2, strides=(2, 2, 2), padding='same')(x)\n        merge = Concatenate()([up, skip])\n        c = conv_block(merge, filters)\n        return c\n\n    c1, p1 = encoder_block(inputs, 32)\n    c2, p2 = encoder_block(p1, 64)\n    c3, p3 = encoder_block(p2, 128)\n    c4, p4 = encoder_block(p3, 256)\n\n    bn = conv_block(p4, 512)\n\n    d4 = decoder_block(bn, c4, 256)\n    d4 = se_block(d4)\n\n    d3 = decoder_block(d4, c3, 128)\n    d3 = se_block(d3)\n\n    d2 = decoder_block(d3, c2, 64)\n    d2 = se_block(d2)\n\n    d1 = decoder_block(d2, c1, 32)\n\n    outputs = Conv3D(out_classes, 1, activation='softmax')(d1)\n\n    return Model(inputs=[inputs], outputs=[outputs])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageSequence(Sequence):\n    def __init__(self, img_dir, mask_dir, batch_size):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.batch_size = batch_size\n        self.img_files = sorted(os.listdir(img_dir))\n        self.mask_files = sorted(os.listdir(mask_dir))\n        self.indices = np.arange(len(self.img_files))\n\n    def __len__(self):\n        return int(np.ceil(len(self.img_files) / float(self.batch_size)))\n\n    def __getitem__(self, index):\n        batch_img = []\n        batch_mask = []\n\n        for i in range(index * self.batch_size, min((index + 1) * self.batch_size, len(self.img_files))):\n            img = np.load(os.path.join(self.img_dir, self.img_files[i])).astype(np.float32)\n            img = img / np.max(img)  # Normalize\n\n            mask = np.load(os.path.join(self.mask_dir, self.mask_files[i]))\n            if mask.ndim == 3:\n                mask = tf.keras.utils.to_categorical(mask, num_classes=4)\n\n            batch_img.append(img)\n            batch_mask.append(mask)\n\n        return np.array(batch_img), np.array(batch_mask)\n\n    def on_epoch_end(self):\n        np.random.shuffle(self.indices)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 2\nsteps_per_epoch = len(os.listdir(train_img_dir)) // batch_size\ntrain_generator = ImageSequence(train_img_dir, train_mask_dir, batch_size)\n\n\n# Load or initialize model\nif os.path.exists(input_model_path):\n    print(\"Loading pretrained model...\")\n    model = tf.keras.models.load_model(input_model_path, custom_objects={\"loss\": dice_ce_loss()})\n    model.compile(optimizer=Adam(1e-4), loss=dice_ce_loss(), metrics=['accuracy'])  # Recompile after loading\n\nelse:\n    print(\"Creating new LATUP-Net...\")\n    model = latup_attention_unet(128, 128, 128, 4, 4)\n    model.compile(optimizer=Adam(1e-4), loss=dice_ce_loss(), metrics=['accuracy'])\n\n# Callbacks\ncallbacks = [\n    ModelCheckpoint(output_model_path, monitor='loss', save_best_only=True, verbose=1),\n    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1),\n    EarlyStopping(monitor='loss', patience=10, verbose=1)\n]\n\n# Train\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=50,\n    callbacks=callbacks,\n    verbose=1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_img_dir = \"/kaggle/input/training-1111/training/images\"\ntrain_mask_dir = \"/kaggle/input/training-1111/training/masks\"\ninput_model_path = \"/kaggle/input/train-2-30/latup_attention_model_trained_2.h5\"\noutput_model_path = \"/kaggle/working/latup_attention_model_trained_3.h5\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 2\nsteps_per_epoch = len(os.listdir(train_img_dir)) // batch_size\ntrain_generator = ImageSequence(train_img_dir, train_mask_dir, batch_size)\n\n\n# Load or initialize model\nif os.path.exists(input_model_path):\n    print(\"Loading pretrained model...\")\n    model = tf.keras.models.load_model(input_model_path, custom_objects={\"loss\": dice_ce_loss()})\n    model.compile(optimizer=Adam(1e-4), loss=dice_ce_loss(), metrics=['accuracy'])  # Recompile after loading\n\nelse:\n    print(\"Creating new LATUP-Net...\")\n    model = latup_attention_unet(128, 128, 128, 4, 4)\n    model.compile(optimizer=Adam(1e-4), loss=dice_ce_loss(), metrics=['accuracy'])\n\n# Callbacks\ncallbacks = [\n    ModelCheckpoint(output_model_path, monitor='loss', save_best_only=True, verbose=1),\n    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1),\n    EarlyStopping(monitor='loss', patience=10, verbose=1)\n]\n\n# Train\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=70,\n    callbacks=callbacks,\n    verbose=1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# LATUP‑Net – Evaluation Script\n# ===============================\nimport os, glob, numpy as np, tensorflow as tf\nfrom tensorflow.keras.models import load_model\nimport pandas as pd\n\n# --------------------------------------------------\n# 1.  CUSTOM LOSS (needed only for model loading)\n# --------------------------------------------------\ndef dice_ce_loss(smooth=1e-6):\n    def loss(y_true, y_pred):\n        weights = tf.constant([0, 0.4, 0.25, 0.45], dtype=tf.float32)\n        y_true  = tf.cast(y_true, tf.float32)\n        y_pred  = tf.cast(y_pred, tf.float32)\n        y_pred  = tf.clip_by_value(\n            y_pred, tf.keras.backend.epsilon(), 1. - tf.keras.backend.epsilon()\n        )\n        ce = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true, y_pred))\n        dice_terms = []\n        for c in range(y_true.shape[-1]):\n            inter = tf.reduce_sum(y_true[..., c] * y_pred[..., c])\n            denom = tf.reduce_sum(y_true[..., c] + y_pred[..., c])\n            dice  = (2. * inter + smooth) / (denom + smooth)\n            dice_terms.append((1 - dice) * weights[c])\n        dice_loss = tf.reduce_sum(dice_terms)\n        return dice_loss + ce\n    return loss\n\n# --------------------------------------------------\n# 2.  LOAD TRAINED MODEL\n# --------------------------------------------------\n\nmodel = load_model(\n    \"/kaggle/input/train-1-30/latup_attention_model_1_3 (2).h5\",\n    custom_objects={\"dice_ce_loss\": dice_ce_loss},\n    compile=False\n)\n\nmodel.trainable = False  # keep everything in inference mode\n\n# --------------------------------------------------\n# 3.  PRE‑ / POST‑PROCESS HELPERS\n# --------------------------------------------------\ndef zscore(volume):\n    \"\"\"Per‑modality z‑normalisation for a 4‑D MRI block (x,y,z,channels).\"\"\"\n    vol = volume.astype(np.float32)\n    for c in range(vol.shape[-1]):\n        mu  = vol[..., c].mean()\n        sig = vol[..., c].std()\n        vol[..., c] = (vol[..., c] - mu) / (sig + 1e-8)\n    return vol\n\ndef dice_coef(mask_gt, mask_pred, label, smooth=1e-6):\n    \"\"\"Plain single‑label Dice (inputs are binary: 0/1).\"\"\"\n    gt   = (mask_gt  == label).astype(np.float32)\n    pred = (mask_pred == label).astype(np.float32)\n    inter = np.sum(gt * pred)\n    denom = np.sum(gt) + np.sum(pred)\n    return (2 * inter + smooth) / (denom + smooth)\n\ndef composite_labels(mask):\n    \"\"\"Return 3 composite binary masks for WT, TC, ET.\"\"\"\n    # BraTS labels: 0 = bg, 1 = edema, 2 = non‑enh core, 3 = enh core\n    wt = np.isin(mask, [1,2,3])\n    tc = np.isin(mask, [2,3])\n    et = (mask == 3)\n    return wt, tc, et\n\n# --------------------------------------------------\n# 4.  DATA LOADING (same folder layout as training)\n# --------------------------------------------------\ntest_img_dir  = \"/kaggle/input/testing/testing/images\"\ntest_mask_dir = \"/kaggle/input/testing/testing/masks\"\n\nimage_paths = sorted(glob.glob(os.path.join(test_img_dir,  \"*.npy\")))\nmask_paths  = sorted(glob.glob(os.path.join(test_mask_dir, \"*.npy\")))\nassert len(image_paths) == len(mask_paths), \"Image/mask count mismatch\"\n\nresults = []   # will become a dataframe later\n\n   # --------------------------------------------------\n# 5. INFERENCE LOOP\n# --------------------------------------------------\nfor img_p, msk_p in zip(image_paths, mask_paths):\n    case_id = os.path.basename(img_p).replace(\".npy\", \"\")\n    img  = np.load(img_p)   # (X,Y,Z,4)\n    mask = np.load(msk_p)   # (X,Y,Z) or one-hot\n\n    if mask.ndim == 4:  # one-hot to label\n        mask = np.argmax(mask, axis=-1)\n\n    img_norm = zscore(img)[None, ...]\n    pred_prob = model.predict(img_norm, verbose=0)[0]\n    pred_lbl  = np.argmax(pred_prob, axis=-1)\n\n    # Label‑wise Dice scores\n    d_bg  = dice_coef(mask, pred_lbl, 0)\n    d_ed  = dice_coef(mask, pred_lbl, 1)  # Edema\n    d_nec = dice_coef(mask, pred_lbl, 2)  # Necrosis (non-enh core)\n    d_et  = dice_coef(mask, pred_lbl, 3)  # Enhancing tumor\n\n    # Composite Dice scores\n    wt_gt, tc_gt, et_gt = composite_labels(mask)\n    wt_pr, tc_pr, et_pr = composite_labels(pred_lbl)\n    d_wt = dice_coef(wt_gt, wt_pr, True)\n    d_tc = dice_coef(tc_gt, tc_pr, True)\n    d_enh = dice_coef(et_gt, et_pr, True)\n\n    results.append(dict(\n        case=case_id,\n        Dice_BG=d_bg,\n        Dice_Edema=d_ed,\n        Dice_Necrosis=d_nec,\n        Dice_EnhancingTumor=d_et,\n        Dice_WT=d_wt,\n        Dice_TC=d_tc,\n        Dice_ET=d_enh\n    ))\n\n    print(f\"{case_id}: WT {d_wt:.4f} | TC {d_tc:.4f} | ET {d_enh:.4f} | BG {d_bg:.4f} | ED {d_ed:.4f} | NEC {d_nec:.4f} | ETum {d_et:.4f}\")\n\n# --------------------------------------------------\n# 6. SUMMARY REPORTING\n# --------------------------------------------------\ndf = pd.DataFrame(results)\n\nif df.empty:\n    print(\"\\n❌ No cases processed. Check file paths/extensions.\")\nelse:\n    composite_metrics = [\"Dice_WT\", \"Dice_TC\", \"Dice_ET\"]\n    label_metrics     = [\"Dice_BG\", \"Dice_Edema\", \"Dice_Necrosis\", \"Dice_EnhancingTumor\"]\n\n    print(\"\\n=== Mean Dice Scores: Composite Regions ===\")\n    print(df[composite_metrics].mean())\n\n    print(\"\\n=== Mean Dice Scores: Per-Class ===\")\n    print(df[label_metrics].mean())\n\n    print(\"\\nDetailed per-case Dice table:\")\n    print(df)\n\n    # Optional CSV export\n    # df.to_csv(\"latupnet_dice_scores.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T06:37:09.479378Z","iopub.execute_input":"2025-07-26T06:37:09.479785Z","iopub.status.idle":"2025-07-26T06:39:25.360577Z","shell.execute_reply.started":"2025-07-26T06:37:09.479738Z","shell.execute_reply":"2025-07-26T06:39:25.359845Z"}},"outputs":[{"name":"stderr","text":"2025-07-26 06:37:10.955745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753511831.141921      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753511831.194155      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1753511843.450898      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1753511847.024132      93 service.cc:148] XLA service 0x7f15c03126e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1753511847.024994      93 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1753511847.236737      93 cuda_dnn.cc:529] Loaded cuDNN version 90300\nE0000 00:00:1753511847.965341      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511848.204452      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511848.495397      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511848.697071      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511849.193875      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511849.443742      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511850.497062      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511850.779419      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511853.301327      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511853.572901      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511854.117904      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511854.380936      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511854.658560      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1753511854.830950      93 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nI0000 00:00:1753511856.101604      93 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"image_294: WT 0.9278 | TC 0.6726 | ET 0.0000 | BG 0.9991 | ED 0.0000 | NEC 0.6552 | ETum 0.0000\nimage_295: WT 0.9693 | TC 0.6806 | ET 0.2465 | BG 0.9974 | ED 0.0506 | NEC 0.5975 | ETum 0.2465\nimage_296: WT 0.6895 | TC 0.5110 | ET 0.0000 | BG 0.9913 | ED 0.1428 | NEC 0.5083 | ETum 0.0000\nimage_297: WT 0.8518 | TC 0.6570 | ET 0.0364 | BG 0.9973 | ED 0.0000 | NEC 0.6562 | ETum 0.0364\nimage_298: WT 0.4296 | TC 0.3159 | ET 0.0000 | BG 0.9801 | ED 0.0000 | NEC 0.3197 | ETum 0.0000\nimage_299: WT 0.8650 | TC 0.3360 | ET 0.7133 | BG 0.9943 | ED 0.0794 | NEC 0.3240 | ETum 0.7133\nimage_300: WT 0.8342 | TC 0.6257 | ET 0.4764 | BG 0.9869 | ED 0.4508 | NEC 0.6264 | ETum 0.4764\nimage_301: WT 0.8184 | TC 0.1043 | ET 0.6434 | BG 0.9914 | ED 0.2738 | NEC 0.0549 | ETum 0.6434\nimage_302: WT 0.7165 | TC 0.5917 | ET 0.4509 | BG 0.9963 | ED 0.0065 | NEC 0.5466 | ETum 0.4509\nimage_303: WT 0.8421 | TC 0.1126 | ET 1.0000 | BG 0.9977 | ED 0.0000 | NEC 0.1126 | ETum 1.0000\nimage_304: WT 0.8325 | TC 0.4968 | ET 0.0000 | BG 0.9912 | ED 0.5709 | NEC 0.5047 | ETum 0.0000\nimage_305: WT 0.8411 | TC 0.5129 | ET 0.0000 | BG 0.9871 | ED 0.2777 | NEC 0.5119 | ETum 0.0000\nimage_306: WT 0.9084 | TC 0.5172 | ET 0.0000 | BG 0.9936 | ED 0.6884 | NEC 0.5176 | ETum 0.0000\nimage_307: WT 0.9065 | TC 0.4455 | ET 0.6507 | BG 0.9908 | ED 0.2752 | NEC 0.4159 | ETum 0.6507\nimage_308: WT 0.8349 | TC 0.3407 | ET 0.4779 | BG 0.9886 | ED 0.1872 | NEC 0.3353 | ETum 0.4779\nimage_309: WT 0.8466 | TC 0.4619 | ET 0.0000 | BG 0.9878 | ED 0.0000 | NEC 0.4678 | ETum 0.0000\nimage_310: WT 0.6530 | TC 0.4754 | ET 0.5956 | BG 0.9961 | ED 0.0000 | NEC 0.4663 | ETum 0.5956\nimage_311: WT 0.9146 | TC 0.6427 | ET 0.0000 | BG 0.9934 | ED 0.0142 | NEC 0.6431 | ETum 0.0000\nimage_312: WT 0.9398 | TC 0.6264 | ET 0.5513 | BG 0.9945 | ED 0.0014 | NEC 0.6230 | ETum 0.5513\nimage_314: WT 0.8537 | TC 0.3676 | ET 0.0000 | BG 0.9975 | ED 0.0000 | NEC 0.3678 | ETum 0.0000\nimage_315: WT 0.8596 | TC 0.6350 | ET 0.4491 | BG 0.9846 | ED 0.0828 | NEC 0.6372 | ETum 0.4491\nimage_317: WT 0.8085 | TC 0.3492 | ET 0.1863 | BG 0.9822 | ED 0.0483 | NEC 0.3358 | ETum 0.1863\nimage_318: WT 0.9466 | TC 0.4803 | ET 0.0000 | BG 0.9968 | ED 0.5525 | NEC 0.4746 | ETum 0.0000\nimage_319: WT 0.7461 | TC 0.6889 | ET 0.5958 | BG 0.9963 | ED 0.4963 | NEC 0.5493 | ETum 0.5958\nimage_320: WT 0.7834 | TC 0.5677 | ET 0.0000 | BG 0.9941 | ED 0.0000 | NEC 0.5688 | ETum 0.0000\nimage_321: WT 0.9497 | TC 0.4095 | ET 0.2055 | BG 0.9977 | ED 0.1009 | NEC 0.4079 | ETum 0.2055\nimage_322: WT 0.8318 | TC 0.5133 | ET 0.0000 | BG 0.9825 | ED 0.0000 | NEC 0.4939 | ETum 0.0000\nimage_323: WT 0.7295 | TC 0.1759 | ET 1.0000 | BG 0.9929 | ED 0.0000 | NEC 0.1759 | ETum 1.0000\nimage_325: WT 0.9020 | TC 0.0244 | ET 0.3671 | BG 0.9946 | ED 0.2292 | NEC 0.0000 | ETum 0.3671\nimage_326: WT 0.7340 | TC 0.3807 | ET 0.4369 | BG 0.9807 | ED 0.0084 | NEC 0.2210 | ETum 0.4369\nimage_327: WT 0.8213 | TC 0.5190 | ET 0.5306 | BG 0.9703 | ED 0.0223 | NEC 0.5170 | ETum 0.5306\nimage_328: WT 0.8846 | TC 0.3185 | ET 0.0000 | BG 0.9966 | ED 0.0000 | NEC 0.3185 | ETum 0.0000\nimage_329: WT 0.9457 | TC 0.5052 | ET 0.0000 | BG 0.9986 | ED 0.0000 | NEC 0.5053 | ETum 0.0000\nimage_330: WT 0.8602 | TC 0.7668 | ET 0.4580 | BG 0.9961 | ED 0.0007 | NEC 0.7468 | ETum 0.4580\nimage_331: WT 0.9502 | TC 0.9460 | ET 0.5235 | BG 0.9986 | ED 0.0000 | NEC 0.9369 | ETum 0.5235\nimage_332: WT 0.9404 | TC 0.1364 | ET 0.0196 | BG 0.9953 | ED 0.0000 | NEC 0.0472 | ETum 0.0196\nimage_333: WT 0.9594 | TC 0.9558 | ET 0.8233 | BG 0.9968 | ED 0.0360 | NEC 0.9412 | ETum 0.8233\nimage_334: WT 0.9055 | TC 0.6317 | ET 0.0000 | BG 0.9939 | ED 0.0142 | NEC 0.6322 | ETum 0.0000\nimage_335: WT 0.9515 | TC 0.9082 | ET 0.9022 | BG 0.9983 | ED 0.9568 | NEC 0.8600 | ETum 0.9022\nimage_336: WT 0.9525 | TC 0.9242 | ET 0.9065 | BG 0.9977 | ED 0.9433 | NEC 0.8851 | ETum 0.9065\nimage_337: WT 0.9129 | TC 0.8884 | ET 0.9211 | BG 0.9938 | ED 0.9019 | NEC 0.8373 | ETum 0.9211\nimage_338: WT 0.8829 | TC 0.8463 | ET 0.9096 | BG 0.9914 | ED 0.9440 | NEC 0.7953 | ETum 0.9096\nimage_339: WT 0.8673 | TC 0.8239 | ET 0.8897 | BG 0.9943 | ED 0.9002 | NEC 0.7220 | ETum 0.8897\nimage_341: WT 0.8647 | TC 0.7371 | ET 0.8295 | BG 0.9900 | ED 0.9199 | NEC 0.5360 | ETum 0.8295\nimage_342: WT 0.9352 | TC 0.8988 | ET 0.8565 | BG 0.9950 | ED 0.8882 | NEC 0.8481 | ETum 0.8565\nimage_343: WT 0.9023 | TC 0.8650 | ET 0.6101 | BG 0.9960 | ED 0.3334 | NEC 0.8321 | ETum 0.6101\nimage_344: WT 0.8375 | TC 0.8372 | ET 0.8190 | BG 0.9935 | ED 0.0274 | NEC 0.8243 | ETum 0.8190\nimage_345: WT 0.9562 | TC 0.8909 | ET 0.8967 | BG 0.9983 | ED 0.9778 | NEC 0.8234 | ETum 0.8967\nimage_346: WT 0.9181 | TC 0.9067 | ET 0.9310 | BG 0.9951 | ED 0.9009 | NEC 0.8859 | ETum 0.9310\nimage_347: WT 0.9507 | TC 0.9082 | ET 0.8763 | BG 0.9979 | ED 0.8480 | NEC 0.7861 | ETum 0.8763\nimage_348: WT 0.9568 | TC 0.9119 | ET 0.8714 | BG 0.9965 | ED 0.9305 | NEC 0.8477 | ETum 0.8714\nimage_349: WT 0.9314 | TC 0.9152 | ET 0.9414 | BG 0.9965 | ED 0.9205 | NEC 0.8801 | ETum 0.9414\nimage_350: WT 0.9358 | TC 0.9307 | ET 0.9523 | BG 0.9968 | ED 0.9038 | NEC 0.9163 | ETum 0.9523\nimage_351: WT 0.9382 | TC 0.9294 | ET 0.8319 | BG 0.9989 | ED 0.3041 | NEC 0.9074 | ETum 0.8319\nimage_352: WT 0.9180 | TC 0.8941 | ET 0.8289 | BG 0.9986 | ED 0.9024 | NEC 0.8958 | ETum 0.8289\nimage_353: WT 0.9312 | TC 0.8917 | ET 0.9153 | BG 0.9940 | ED 0.9063 | NEC 0.8475 | ETum 0.9153\nimage_354: WT 0.0000 | TC 0.0000 | ET 0.0000 | BG 0.9567 | ED 0.0000 | NEC 0.0000 | ETum 0.0000\nimage_355: WT 0.0000 | TC 0.0000 | ET 0.0000 | BG 0.9791 | ED 0.0000 | NEC 0.0000 | ETum 0.0000\nimage_356: WT 0.3055 | TC 0.2622 | ET 0.0000 | BG 0.9676 | ED 0.0000 | NEC 0.2396 | ETum 0.0000\nimage_357: WT 0.5428 | TC 0.4440 | ET 0.1178 | BG 0.9777 | ED 0.0104 | NEC 0.2826 | ETum 0.1178\nimage_358: WT 0.0090 | TC 0.0108 | ET 0.0000 | BG 0.9532 | ED 0.0000 | NEC 0.0193 | ETum 0.0000\nimage_359: WT 0.0000 | TC 0.0000 | ET 0.0000 | BG 0.9578 | ED 0.0000 | NEC 0.0000 | ETum 0.0000\nimage_360: WT 0.0938 | TC 0.1040 | ET 0.0000 | BG 0.9515 | ED 0.0000 | NEC 0.1288 | ETum 0.0000\nimage_361: WT 0.6587 | TC 0.5413 | ET 0.2995 | BG 0.9757 | ED 0.3395 | NEC 0.3816 | ETum 0.2995\nimage_362: WT 0.0000 | TC 0.0000 | ET 0.0000 | BG 0.9449 | ED 0.0000 | NEC 0.0000 | ETum 0.0000\nimage_363: WT 0.1656 | TC 0.1413 | ET 0.0002 | BG 0.9487 | ED 0.0000 | NEC 0.0820 | ETum 0.0002\nimage_364: WT 0.0000 | TC 0.0000 | ET 0.0000 | BG 0.9482 | ED 0.0000 | NEC 0.0000 | ETum 0.0000\nimage_365: WT 0.0000 | TC 0.0000 | ET 0.0000 | BG 0.9638 | ED 0.0000 | NEC 0.0000 | ETum 0.0000\nimage_366: WT 0.0000 | TC 0.0000 | ET 0.0000 | BG 0.9479 | ED 0.0000 | NEC 0.0000 | ETum 0.0000\nimage_367: WT 0.2270 | TC 0.2265 | ET 0.1416 | BG 0.9430 | ED 0.0192 | NEC 0.1774 | ETum 0.1416\n\n=== Mean Dice Scores: Composite Regions ===\nDice_WT    0.722562\nDice_TC    0.516245\nDice_ET    0.395523\ndtype: float64\n\n=== Mean Dice Scores: Per-Class ===\nDice_BG                0.986275\nDice_Edema             0.276983\nDice_Necrosis          0.485800\nDice_EnhancingTumor    0.395523\ndtype: float64\n\nDetailed per-case Dice table:\n         case   Dice_BG    Dice_Edema  Dice_Necrosis  Dice_EnhancingTumor  \\\n0   image_294  0.999138  9.756098e-11   6.551688e-01         1.930502e-09   \n1   image_295  0.997397  5.057127e-02   5.974703e-01         2.464846e-01   \n2   image_296  0.991312  1.427648e-01   5.082514e-01         1.764914e-10   \n3   image_297  0.997347  7.907639e-11   6.561563e-01         3.636365e-02   \n4   image_298  0.980094  1.148409e-11   3.197274e-01         1.517451e-09   \n..        ...       ...           ...            ...                  ...   \n65  image_363  0.948697  1.900924e-11   8.203110e-02         1.749078e-04   \n66  image_364  0.948204  2.928429e-11   7.756568e-12         2.299961e-11   \n67  image_365  0.963766  1.595660e-10   8.923074e-12         3.530076e-11   \n68  image_366  0.947873  2.544529e-10   5.913136e-12         2.877036e-11   \n69  image_367  0.943029  1.917246e-02   1.773508e-01         1.416274e-01   \n\n         Dice_WT       Dice_TC       Dice_ET  \n0   9.277537e-01  6.726373e-01  1.930502e-09  \n1   9.693037e-01  6.805779e-01  2.464846e-01  \n2   6.895374e-01  5.110391e-01  1.764914e-10  \n3   8.518062e-01  6.570368e-01  3.636365e-02  \n4   4.295875e-01  3.159250e-01  1.517451e-09  \n..           ...           ...           ...  \n65  1.655910e-01  1.412930e-01  1.749078e-04  \n66  4.841443e-12  5.800397e-12  2.299961e-11  \n67  6.818306e-12  7.122659e-12  3.530076e-11  \n68  4.812250e-12  4.905014e-12  2.877036e-11  \n69  2.269931e-01  2.264695e-01  1.416274e-01  \n\n[70 rows x 8 columns]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Input, Conv3D, MaxPooling3D, Conv3DTranspose, Concatenate,\n                                     BatchNormalization, Activation, Dropout, Add, Multiply,\n                                     GlobalAveragePooling3D, Dense, Reshape)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import Sequence\n\ntrain_img_dir = \"/kaggle/input/training-1111/training/images\"\ntrain_mask_dir = \"/kaggle/input/training-1111/training/masks\"\ntest_img_dir = \"/kaggle/input/testing/testing/images\"\ntest_mask_dir = \"/kaggle/input/testing/testing/masks\"\ninput_model_path = \"/kaggle/input/train-1-30/latup_attention_model_1_3 (2).h5\"\noutput_model_path = \"/kaggle/working/latup_attention_model_trained_final_1.h5\"\n\n# === Loss Function ===\ndef weighted_dice_ce_loss(smooth=1e-6):\n    class_weights = tf.constant([0.05, 0.35, 0.30, 0.30], dtype=tf.float32)  # [BG, Edema, Necrosis, ET]\n\n    def loss(y_true, y_pred):\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n\n        # Dice Loss per class\n        axes = [1, 2, 3]  # skip batch and channels\n        intersection = tf.reduce_sum(y_true * y_pred, axis=axes)\n        denominator = tf.reduce_sum(y_true + y_pred, axis=axes)\n        dice = (2. * intersection + smooth) / (denominator + smooth)\n        dice_loss = 1.0 - dice\n        weighted_dice_loss = tf.reduce_mean(dice_loss * class_weights)\n\n        # Weighted CE Loss\n        ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)  # shape [batch, depth, height, width]\n        class_map = tf.reduce_sum(y_true * class_weights, axis=-1)  # shape [batch, depth, height, width]\n        weighted_ce_loss = tf.reduce_mean(ce_loss * class_map)\n\n        return weighted_dice_loss + weighted_ce_loss\n\n    return loss\n\n# === Per-Class Dice Metrics ===\ndef compute_per_class_dice(y_true, y_pred, smooth=1e-6):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n    y_true = tf.argmax(y_true, axis=-1)\n    dice_scores = []\n    for i in range(4):\n        y_t = tf.cast(tf.equal(y_true, i), tf.float32)\n        y_p = tf.cast(tf.equal(y_pred, i), tf.float32)\n        intersection = tf.reduce_sum(y_t * y_p)\n        denominator = tf.reduce_sum(y_t + y_p)\n        dice = (2. * intersection + smooth) / (denominator + smooth)\n        dice_scores.append(dice.numpy())\n    return dice_scores\n\nclass PerClassDiceCallback(tf.keras.callbacks.Callback):\n    def __init__(self, test_gen, interval=10):\n        super().__init__()\n        self.test_gen = test_gen\n        self.interval = interval\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.interval == 0:\n            print(f\"\\n=== Epoch {epoch + 1}: Evaluating per-class Dice on test set ===\")\n            all_scores = []\n            for i in range(len(self.test_gen)):\n                x_batch, y_batch = self.test_gen[i]\n                y_pred = self.model.predict(x_batch, verbose=0)\n                scores = compute_per_class_dice(y_batch, y_pred)\n                all_scores.append(scores)\n\n            print(f\"Dice_BG: {mean_scores[0]:.6f}\")\n            print(f\"Dice_Edema: {mean_scores[1]:.6f}\")\n            print(f\"Dice_Necrosis: {mean_scores[2]:.6f}\")\n            print(f\"Dice_EnhancingTumor: {mean_scores[3]:.6f}\")\n\n# === Model Definition ===\ndef se_block(input_tensor, reduction=16):\n    channels = input_tensor.shape[-1]\n    se = GlobalAveragePooling3D()(input_tensor)\n    se = Dense(channels // reduction, activation='relu')(se)\n    se = Dense(channels, activation='sigmoid')(se)\n    se = Reshape((1, 1, 1, channels))(se)\n    return Multiply()([input_tensor, se])\n\ndef latup_attention_unet(x, y, z, in_channels, out_classes):\n    inputs = Input((x, y, z, in_channels))\n    def conv_block(x, filters):\n        x = Conv3D(filters, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Conv3D(filters, 3, padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        return x\n    def encoder_block(x, filters):\n        c = conv_block(x, filters)\n        p = MaxPooling3D(pool_size=(2, 2, 2))(c)\n        return c, p\n    def decoder_block(x, skip, filters):\n        up = Conv3DTranspose(filters, 2, strides=(2, 2, 2), padding='same')(x)\n        merge = Concatenate()([up, skip])\n        c = conv_block(merge, filters)\n        return c\n    c1, p1 = encoder_block(inputs, 32)\n    c2, p2 = encoder_block(p1, 64)\n    c3, p3 = encoder_block(p2, 128)\n    c4, p4 = encoder_block(p3, 256)\n    bn = conv_block(p4, 512)\n    d4 = decoder_block(bn, c4, 256); d4 = se_block(d4)\n    d3 = decoder_block(d4, c3, 128); d3 = se_block(d3)\n    d2 = decoder_block(d3, c2, 64);  d2 = se_block(d2)\n    d1 = decoder_block(d2, c1, 32)\n    outputs = Conv3D(out_classes, 1, activation='softmax')(d1)\n    return Model(inputs=[inputs], outputs=[outputs])\n\n# === Data Loader ===\nclass ImageSequence(Sequence):\n    def __init__(self, img_dir, mask_dir, batch_size):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.batch_size = batch_size\n        self.img_files = sorted(os.listdir(img_dir))\n        self.mask_files = sorted(os.listdir(mask_dir))\n        self.indices = np.arange(len(self.img_files))\n    def __len__(self):\n        return int(np.ceil(len(self.img_files) / float(self.batch_size)))\n    def __getitem__(self, index):\n        batch_img, batch_mask = [], []\n        for i in range(index * self.batch_size, min((index + 1) * self.batch_size, len(self.img_files))):\n            img = np.load(os.path.join(self.img_dir, self.img_files[i])).astype(np.float32)\n            img = img / np.max(img)\n            mask = np.load(os.path.join(self.mask_dir, self.mask_files[i]))\n            if mask.ndim == 3:\n                mask = tf.keras.utils.to_categorical(mask, num_classes=4)\n            batch_img.append(img)\n            batch_mask.append(mask)\n        return np.array(batch_img), np.array(batch_mask)\n    def on_epoch_end(self):\n        np.random.shuffle(self.indices)\n\n# === Training ===\nbatch_size = 2\ntrain_generator = ImageSequence(train_img_dir, train_mask_dir, batch_size)\ntest_generator = ImageSequence(test_img_dir, test_mask_dir, batch_size)\nsteps_per_epoch = len(train_generator)\n\n# Load or build model\nif os.path.exists(input_model_path):\n    print(\"Loading pretrained model...\")\n    model = tf.keras.models.load_model(input_model_path, custom_objects={\"loss\": weighted_dice_ce_loss()})\n    model.compile(optimizer=Adam(1e-4), loss=weighted_dice_ce_loss(), metrics=['accuracy'])\nelse:\n    print(\"Creating new LATUP-Net...\")\n    model = latup_attention_unet(128, 128, 128, 4, 4)\n    model.compile(optimizer=Adam(1e-4), loss=weighted_dice_ce_loss(), metrics=['accuracy'])\n\n# Callbacks\ncallbacks = [\n    ModelCheckpoint(output_model_path, monitor='loss', save_best_only=True, verbose=1),\n    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1),\n    EarlyStopping(monitor='loss', patience=10, verbose=1),\n    PerClassDiceCallback(test_generator, interval=10)\n]\n\n# Train\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=20,\n    callbacks=callbacks,\n    verbose=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:50:45.016287Z","iopub.execute_input":"2025-07-26T08:50:45.016557Z","iopub.status.idle":"2025-07-26T09:46:12.805785Z","shell.execute_reply.started":"2025-07-26T08:50:45.016533Z","shell.execute_reply":"2025-07-26T09:46:12.801125Z"}},"outputs":[{"name":"stdout","text":"Loading pretrained model...\nEpoch 1/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9617 - loss: 0.1936\nEpoch 1: loss improved from inf to 0.16795, saving model to /kaggle/working/latup_attention_model_trained_final_1.h5\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 2s/step - accuracy: 0.9617 - loss: 0.1934 - learning_rate: 1.0000e-04\nEpoch 2/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9805 - loss: 0.1376\nEpoch 2: loss improved from 0.16795 to 0.14129, saving model to /kaggle/working/latup_attention_model_trained_final_1.h5\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 2s/step - accuracy: 0.9805 - loss: 0.1376 - learning_rate: 1.0000e-04\nEpoch 3/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9794 - loss: 0.1347\nEpoch 3: loss improved from 0.14129 to 0.13315, saving model to /kaggle/working/latup_attention_model_trained_final_1.h5\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 2s/step - accuracy: 0.9794 - loss: 0.1347 - learning_rate: 1.0000e-04\nEpoch 4/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9816 - loss: 0.1296\nEpoch 4: loss did not improve from 0.13315\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 2s/step - accuracy: 0.9816 - loss: 0.1296 - learning_rate: 1.0000e-04\nEpoch 5/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9829 - loss: 0.1254\nEpoch 5: loss improved from 0.13315 to 0.12361, saving model to /kaggle/working/latup_attention_model_trained_final_1.h5\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 2s/step - accuracy: 0.9829 - loss: 0.1254 - learning_rate: 1.0000e-04\nEpoch 6/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9787 - loss: 0.1351\nEpoch 6: loss did not improve from 0.12361\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 2s/step - accuracy: 0.9787 - loss: 0.1350 - learning_rate: 1.0000e-04\nEpoch 7/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9815 - loss: 0.1233\nEpoch 7: loss did not improve from 0.12361\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 2s/step - accuracy: 0.9815 - loss: 0.1233 - learning_rate: 1.0000e-04\nEpoch 8/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9813 - loss: 0.1210\nEpoch 8: loss improved from 0.12361 to 0.12103, saving model to /kaggle/working/latup_attention_model_trained_final_1.h5\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 2s/step - accuracy: 0.9813 - loss: 0.1210 - learning_rate: 1.0000e-04\nEpoch 9/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9797 - loss: 0.1242\nEpoch 9: loss improved from 0.12103 to 0.11612, saving model to /kaggle/working/latup_attention_model_trained_final_1.h5\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 2s/step - accuracy: 0.9797 - loss: 0.1241 - learning_rate: 1.0000e-04\nEpoch 10/20\n\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9831 - loss: 0.1130\nEpoch 10: loss improved from 0.11612 to 0.11478, saving model to /kaggle/working/latup_attention_model_trained_final_1.h5\n\n=== Epoch 10: Evaluating per-class Dice on test set ===\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/50761098.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/50761098.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mall_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dice_BG: {mean_scores[0]:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dice_Edema: {mean_scores[1]:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dice_Necrosis: {mean_scores[2]:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'mean_scores' is not defined"],"ename":"NameError","evalue":"name 'mean_scores' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}